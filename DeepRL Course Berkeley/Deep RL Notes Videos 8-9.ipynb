{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berkeley Deep RL Course Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Lecture 8\n",
    "\n",
    "---\n",
    "__[Berkeley DeepRL Course Lecture 8 Video](https://youtu.be/BB-BhTn6DCM)__\n",
    "\n",
    "--- \n",
    "**Policy Gradient Methods**\n",
    "\n",
    "\n",
    "#### Policy Gradient Methods\n",
    "\n",
    "In RL the goal is to maximize the sum of the expected rewards over time. \n",
    "- Fixed-horizon episodic (fixed time steps)\n",
    "- Average cost (average reward)\n",
    "- Infinite-horizon discount (sum all rewards and multiply by discount, futher rewards are less important)\n",
    "- Variable-length undiscounted (probability of transitioning to a terminal state- episode can end early)\n",
    "- Infinite-horizon undiscounted (sum is always finite)\n",
    " \n",
    "All yield similiar algorithms.  \n",
    "\n",
    "*Focus: fixed-horizon episodic, other can be expressed as a limiting form of this one.*\n",
    "*Goal: Maximizing the expected value of the sum of rewards.*\n",
    "\n",
    "Can be used with both deterministic and stochastic policy families.\n",
    "\n",
    "The way policies are parameterized is analogous to classification or regression.\n",
    "- Discrete action space: network outputs vector of probabilities\n",
    "- Continuous action space: network outputs mean and diagonal covariance of Gaussian\n",
    "\n",
    "#### Policy Gradient Methods:\n",
    "\n",
    "How can we do this?<br>\n",
    "Collect a bunch of trajectories, try to make the good ones more probable.<br>\n",
    "Better, try to make the good actions more probable.<br>\n",
    "Even better, try to push your actions towards better actions (later topic)<br>\n",
    "\n",
    "Basis of these policy gradient methods is the score function gradient estimator.  Where you want to compute the gradient of some expectation.  <br>\n",
    "\n",
    "Score function gradient estimator for policies--> what we are doing is taking the good trajectories and treateing them like supervised examples in classification or regression.<br>\n",
    "\n",
    "To lower variance use temporal structure: <br>\n",
    "When we get a big reward--> we want to boost the probabilities of the previous actions<br>\n",
    "- can add a baseline--> only boost probability if better than average rewards<br>\n",
    "- introduce a discount factor that ignores delayed effects betweek actions and rewards --> acts like an effective time horizon<br>\n",
    "\n",
    "#### Vanilla Policy Gradient Algorithm \n",
    "\n",
    "1. Do a bunch of batches and each batch you collect a set of trajectories by implementing the current policy.\n",
    "2. At each timestep in the trajectory you compute the return which is the discounted sum of future rewards following that timestep, for the given trajectory.\n",
    "3. Compute the advantage estimate: return-baseline, how much better was that action than the average action.\n",
    "4. Refit the baseline function: baseline should equal the discounted return \n",
    "5. Update the policy using a policy gradient estimate: compute the gradient estimator, plug into sgd or adam.\n",
    "\n",
    "#### Implementation with Autodiff\n",
    "Use neural networks with tensorflow by implementing a loss function.\n",
    "\n",
    "#### Value Functions\n",
    "Q-Function (aka state-value function) expected sum of rewards conditioned on the first state and first action.<br>\n",
    "Advange Function: conditioned on just the first state (Q-function - value function) tells if the action is better or worse than the average action sampled<br>\n",
    "\n",
    "Value functions can be added to policy gradient formulas.  Can plug in an advantage estimators with the form of return - V(s).<br>\n",
    "\n",
    "#### Value Functions in the future\n",
    "The baseline accounts for and removes the effect of past actions, subtracts out the value of the current state that depends on all the past actions.<br>\n",
    "The value function removes the effect of the future actions. By subtracting out the value function at the current timestep, we get the advantage estimator.\n",
    "\n",
    "#### Model Predictive Control Connections\n",
    "MPC look at a finite horizon problem, compute optimal policy at each timestep, take optimal action at first timestep, then replan... next action...<br>\n",
    "Can compute a finite horizon Q-function and take the max over actions.<br>\n",
    "The finite horizon works like a discount factor (finite horizon-hard, discount-soft).<br>\n",
    "\n",
    "#### Paper: Asynchronous methods for deep reinforcement learning (2016)\n",
    "Finite-horizon method: actor-critic<br>\n",
    "Used a fixed-horizon advantage estimator.\n",
    "1.  Agent acts for T timesteps.\n",
    "2.  At each timestep: compute returns (discounted sum of rewards up to the last time step) and advantages (that return minus the state value function aproximated by a neural network)\n",
    "3.  Compute loss gradient- update network\n",
    "4.  Plug gradient estimator into stochastic gradient descent algorithm.\n",
    "\n",
    "Found that the high varience was not as big a problem as previously thought. \n",
    "\n",
    "__[Nice intuitive explanation of policy gradients](http://karpathy.github.io/2016/05/31/rl/)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Lecture 9\n",
    "\n",
    "---\n",
    "__[Berkeley DeepRL Course Lecture 9 Video](https://youtu.be/Wnl-Qh2UHGg)__\n",
    "\n",
    "--- \n",
    "**Q-Function Learning Methods**\n",
    "\n",
    "Use value based algorithms to solve for policies. <br>\n",
    "\n",
    "Model free reinforcement methods that center around value functions- mostly value iteration.<br>\n",
    "\n",
    "Bellman equation is a consistency function.<br>\n",
    "\n",
    "**Bellman Backup**\n",
    "Bellman backup operator: take the right side of bellman equation (looking into the future), maps q-functions to q-functions.  This is what you get if you do 2 steps of finite horizon problem<br>\n",
    "\n",
    "**Q\\* (Q-function of the optimal policy)**<br>\n",
    "pi\\* is the optimal policy for the gamma discounted policy<br>\n",
    "pi\\* is the argmax of the Q\\*(s,a)<br>\n",
    "The value function is just expectations over actions of the Q-function.<br>\n",
    "\n",
    "The bellman backup is a contraction that converges at Q\\*<br>\n",
    "\n",
    "#### Q-Value Iteration\n",
    "Implementing like this would mean storing a bunch of numbers- but for simplicity.<br>\n",
    "1. initialize Q\n",
    "loop<br>\n",
    "2. Q = TQ  (T is backup operator)\n",
    "3. converges at Q\\*\n",
    "\n",
    "Could aslo be done with Greedy policy (G)<br>\n",
    "1. initialize Q\n",
    "loop<br>\n",
    "2. pi^(n+1) = GQ^n\n",
    "3. Q^(n+1) = Q^pi^(n+1)\n",
    "3. converges at Q\\*\n",
    "\n",
    "**Q-Modified Policy Iteration**<br>\n",
    "Instead of solving for the exact Q-function of the policy, you apply k- bellman backups applying t^pi<br>\n",
    "\n",
    "#### Sample Based Estimates\n",
    "\n",
    "We use the two Bellman backup formulas to estimate/calculate Q^pi and Q\\*, then can apply value iteration.<br>\n",
    "Q-functions allow an unbiased estimator for the right hand side of these equations using a single sample, does not matter what policy was used to select the actions.<br>\n",
    "This can be expanded to multi-step estimates.  You can estimate a trajectory or trajectory segment.<br>\n",
    "\n",
    "*Why are we using Q-functions instead of value functions?*<br>\n",
    "We are making a model free: we can get an unbiased estimate for the right hand side of the Q-functions by taking one state transition.\n",
    "- the greedy action can be computed without knowing the probability\n",
    "- can compute an unbiased estimator of backup value without knowing P, using a single transition\n",
    "- can compute unbiased estimator of backup value using off-policy data\n",
    "\n",
    "Sampling-based algorithms\n",
    "1. start with Q-value/policy iteration \n",
    "2. replace backup by estimator\n",
    "3. add some noise to the backup\n",
    "\n",
    "#### Least Squares Version of Backup\n",
    "You can rewrite mean as the minimizer of the sum of squared difference.  This can be applied to Q-values to replace the mean.  This is the same, just a different way to express it.\n",
    "\n",
    "#### Partial backups\n",
    "There might be too much noise in previous method.<br>\n",
    "Instead, move a little bit in the direction by taking a gradient step on the squared error (epsilon).<br>\n",
    "If you take a small multiple of a noisy gradient step-it will improve the result.<br>\n",
    "\n",
    "Sampling-Based Q-Value Iteration is a model free method that will converge on the optimal solution.<br>\n",
    "Be careful with parameters- step sizes.<br>\n",
    "\n",
    "#### Function Approximation/Neural-Fitted Algorithms\n",
    "The purpose of this is to do value iteration where we don't know what the transition probability is.  First switch to Q-functions, then derived a Q-value iteration, modified it to minimize the squared bellman error.  Now a function approximator can be substituted for Q-value iteration.<br>\n",
    "\n",
    "We can have a big neural network that approximates the Q-function.  (neural fitted Q-iteration)<br>\n",
    "- sample a bunch of trajectories using current policy (greedy policy for the current q function plus some exploration noise), then update q-policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
