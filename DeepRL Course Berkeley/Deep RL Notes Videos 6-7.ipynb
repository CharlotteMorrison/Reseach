{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berkeley Deep RL Course Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Lecture 6\n",
    "\n",
    "---\n",
    "__[Berkeley DeepRL Course Lecture 6 Video](https://youtu.be/iHugFEgovhc)__\n",
    "\n",
    "--- \n",
    "**Direct Collocation Methods for Trajectory Optimization and Policy Learning**\n",
    "*Igor Mordatch, OpenAI*\n",
    "\n",
    "Forward shooting has issues with small changes at the begining can have large impact on the end result.  It also has a very narrow feasible region- it can't work through infeasible regions or gets stuck in a local minima.  This can be mitigated by initializing from a demo, or adding in some randomness.<br>\n",
    "\n",
    "Collocation: simultaneously optimized for actions and states, with constraints.<br>\n",
    "Direct collocation: uses an inverse dynamics function to have the states rely on their neighbors, not on all the following states.  No instabilities from forward integration, not a forward dynamics function anymore.  Satisfaction of dynamics is an explicit constraint (can be hard or soft), so less prone to local minima.<br>\n",
    "\n",
    "#### Numerical Solutions for Direct Collocation Methods\n",
    "- Set up a TensorFlow graph and optimize with gradient descent, convergence is slow, instead:\n",
    "- Use a truncated 2nd order method (Iterative LQR, DDP)\n",
    " - Gauss-Newton Method: get trajectory, get the gradient and Hessian, get the solution by iterative Gauss-newton steps.\n",
    " \n",
    "#### Dynamics with contact\n",
    "Shooting and collocation methods work for movement without contact, with contact (manipulation, walking, etc.) either method is hard to apply.<br>\n",
    "Jumps and contact forces may be discontinuous.<br>\n",
    "No gradient for inactive contacts, can't understand that new forces will be applied.<br>\n",
    "\n",
    "Contact activity is an indirect function of state. Make contact activity a direct optimization variable.<br>\n",
    "\n",
    "#### Contact-invariant optimization\n",
    "State also contains contact decision varibles that tell if a part is in contact with something.  These are independet of the poses, can be decided independent of the pose.  Must have consistence for contact variable, penalties are added for forces that are not wanted.  This removes discontinuities because of contact.<br>\n",
    "Start out very soft (few rules for movement) and start enforcing them until you get motion where all forces are plausible and all forces agree with the poses.\n",
    "\n",
    "#### Learning Policies from Trajectory Optimization\n",
    "Trajectory optimization only solves a particular problem. Need to learn control policies to generalize the learning through imitation of optimal control.  <br>\n",
    "Some policies may be difficult to learn because they are very inconsistent, sometimes splitting between 2 policies may create a problem (starting w/left or right- can't use the average).<br>\n",
    "Can add auxillary variables, enforce soft consistency between variables, search in the larger/easier space.\n",
    "\n",
    "_Policy is not converging_\n",
    "Errors are not independent, and can cause drift over time. <br>\n",
    "Policy should be using the neighborhood to determine how the controls should change to maintain optimal trajectory.  Inject noise into the states and then the policy will always converge.<br>\n",
    "\n",
    "Decompose alternating optimization into:\n",
    "- trajectory optimizations\n",
    "- regression\n",
    "\n",
    "#### Unkown/Uncertain Dynamics Applications\n",
    "If the actual robots specs are not perfeclty know, noise can be added to the models and optimized over multiple state trajectories. This can generate a better policy that is more likely to succeed when taken from the simulation to the real world.  If there is too much noise, it will not do anything- becomes so conservative that it can't move.   You can also have the policy update during the running... ask what the next step should look like after the next action, then analyze what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Lecture 7\n",
    "\n",
    "---\n",
    "__[Berkeley DeepRL Course Lecture 7 Video](https://youtu.be/IL3gVyJMmhg)__\n",
    "\n",
    "--- \n",
    "**Markov Decision Processes and Solving Finite problems**\n",
    "Focus: solving without the expert.\n",
    "Assumes a known system with finite states and acions.\n",
    "\n",
    "If you modify value iteration to use small updates and neural nets-> deep Q-network methods\n",
    "If you modify polity iteration to use small updates and neural nets -> deep policy gradient methods\n",
    "\n",
    "#### Markov Decision Process\n",
    "- S: state space (a set of states of the environment)\n",
    "- A: action space (a set of actions which the agent selects from at each timestep)\n",
    "- P: probability of the reward and the next state (transition probablitity distribution)\n",
    "\n",
    "**Partially observed MDP**\n",
    "Frozen Lake: Simple MDP (in openAIGym- gridworld), has start location and goal location.\n",
    "- Gym: FrozenLake-v0\n",
    "- START state, GOAL state, other locations are FROZEN (safe) or HOLE (unsafe)\n",
    "- Episode terminates when GOAL or HOLE state is reached\n",
    "- Reward = 1 for GOAL, 0 otherwise\n",
    "- 4 directions are actions, but you will move in the wrong direction wth probability of .5\n",
    "\n",
    "Policies: how the actions are chosen<br>\n",
    "Deterministic Policies a = pi(s), the action is a function of the state.<br>\n",
    "Stochastic policies a ~ pi(a|s), the action is a conditional distribution given the state.<br>\n",
    "\n",
    "The problems solved with an MDP include: policy optimization- maximize the reward in respect to the policy, policy evaluation- compute the expected return for a given policy. <br>\n",
    "The return is the sum of future rewards in an episode, the discount is the weighting of future rewards.<br>\n",
    "\n",
    "#### Value iteration\n",
    "Value iteration is related to dynamic programming algorithms.  <br>\n",
    "The finite horizon case: the optimal policy might be time dependent.  (LQR is value iteration)\n",
    "Infinite (or super long) time horizon: the discount downweights future rewards and get the discounted return.  Find the policty that will optimized the discounted sum of rewards for each state.  \n",
    "\n",
    "#### Policy iteration\n",
    "Intialize the policy, evaluate it to get the value function, then compute new policy to be greedy version of policy.<br>\n",
    "This will equal the optimal policy and value function for a finite number of iteration.  It converges faster than value iteration.<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
