{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RL Assignment 2: Policy Gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berkeley Deep RL Course Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Lecture 4\n",
    "\n",
    "---\n",
    "__[Berkeley DeepRL Course Lecture 4 Video](https://youtu.be/qVsLk5CVy_c)__\n",
    "\n",
    "### What do we do if the dynamics are unknown?\n",
    "Some systems have very difficult dynamics, very hard to model ahead of time.  You won't know how the state and action lead to the next set.  The model of the dynamics must be learned.\n",
    "\n",
    "#### Overview of model-based RL\n",
    "Why learn the model?\n",
    "Need to know the results of actions. \n",
    "\n",
    "Version 0.5: collect random samples, train dynamics, plan\n",
    "Pro: simple and no iterative process.\n",
    "\n",
    "Classic system identification approach, does it work?\n",
    "Yes... but in reality- no.\n",
    "- how system identification works in classical robotics\n",
    "- must design a good base policy\n",
    "- effective when dynamic representations can be hand engineered using physics and fit just a few parameters.\n",
    "Problems?\n",
    "- distribution mismatch problem: the states we observe at test time are different than the states used to train the dynamics model.\n",
    "- the distribution mismatch problem becomes exacerbated as we use more expressive model classes\n",
    "- the model class is very restrictive in classical robotics so it works there.\n",
    "\n",
    "Version 1.0: iteratively collect data, replan, collect data\n",
    "Can we do better?\n",
    "_Replanning helps with model errors_\n",
    "Errors can accumulate over time steps, this needs to be fixed at each error.  Your model can tell you how much to correct, you can replan your actions at each time steps as you find errors and then relearn the dynamics model over time.\n",
    "It is simple and solves the distribution mismatch but the open loop plan can perform poorly in stochastic domains.\n",
    "\n",
    "Version 1.5: iteratively collet data using MPC (replan at each step)\n",
    "It is robust where there are small errors, but it is computationally expensive and requires a planning algorithm.\n",
    "\n",
    "Version 2.0: backpropagate directly into policy\n",
    "Computationally cheap at runtime, but can be unstable, especially in stochastic domains\n",
    "\n",
    "#### What kind of models can we use?\n",
    "__Gaussian process__\n",
    "Fit a gp where the tuple is state and action and output is next state.\n",
    "They are very data-efficient but not great with non-smooth dynamics, they get slow when the dataset is large.\n",
    "\n",
    "__Neural Network__\n",
    "Input is a tuple of state and action output is next state.\n",
    "Euclidean training loss corresponds to Gaussians, more complex losses.  \n",
    "Very expressive and can use lots of data but are not great with low data regimes.\n",
    "\n",
    "__Other__\n",
    "Gaussian Mixture Model (GMM)\n",
    "Domain specific models\n",
    "Conditional Restricted Boltzmann Machines\n",
    "GPs and GPLVMs\n",
    "Linear and switching linear dynamical systems\n",
    "\n",
    "#### Global models and local models\n",
    "The model based RL will seek out the regions where the model is overly optimistic and you visit all the possible mistakes until you get the right behavior.\n",
    "Must find all the very good models for the state space to converge on a good solution.\n",
    "For easy tasks the model is much more complex than the policy. - instead of fitting a global model, fit a local model in the region you are located.\n",
    "\n",
    "#### Learning with local models and trust regions\n",
    "Local models- calculate new model at each time step.\n",
    "\n",
    "What controller to execute?\n",
    "Version 0.5: doesn't correct for drift or deviations.\n",
    "Version 1.0: gives you one trajectory over and over again.\n",
    "Version 2.0: stochastic, adds noise to vary the states.  How much noise depends on the importance of the step.\n",
    "\n",
    "__local model__\n",
    "- run controllers\n",
    "- collect dynamics\n",
    "- fit dynamics to get derivatives\n",
    "- feed derivatives them to LQR, repeat\n",
    "- fit the dynamics\n",
    "\n",
    "Can we do better?\n",
    "Version 2.0 using Bayesian linear regression using a global model (GP, deep net, GMM)\n",
    "\n",
    "What if we go to far?\n",
    "The linearization is only valid in a local region. We want to keep the error of the controller as small as possible.  \n",
    "The contoller is a Linear Gaussian thing- creates trajectory over time.  We use a trust region.\n",
    "\n",
    "KL divergengces between trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
