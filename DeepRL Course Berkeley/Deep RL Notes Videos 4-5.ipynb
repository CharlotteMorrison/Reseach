{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berkeley Deep RL Course Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Lecture 4\n",
    "\n",
    "---\n",
    "__[Berkeley DeepRL Course Lecture 4 Video](https://youtu.be/qVsLk5CVy_c)__\n",
    "\n",
    "### What do we do if the dynamics are unknown?\n",
    "Some systems have very difficult dynamics, very hard to model ahead of time.  You won't know how the state and action lead to the next set.  The model of the dynamics must be learned.\n",
    "\n",
    "#### Overview of model-based RL\n",
    "Why learn the model?\n",
    "Need to know the results of actions. \n",
    "\n",
    "Version 0.5: collect random samples, train dynamics, plan\n",
    "Pro: simple and no iterative process.\n",
    "\n",
    "Classic system identification approach, does it work?\n",
    "Yes... but in reality- no.\n",
    "- how system identification works in classical robotics\n",
    "- must design a good base policy\n",
    "- effective when dynamic representations can be hand engineered using physics and fit just a few parameters.\n",
    "Problems?\n",
    "- distribution mismatch problem: the states we observe at test time are different than the states used to train the dynamics model.\n",
    "- the distribution mismatch problem becomes exacerbated as we use more expressive model classes\n",
    "- the model class is very restrictive in classical robotics so it works there.\n",
    "\n",
    "Version 1.0: iteratively collect data, replan, collect data\n",
    "Can we do better?\n",
    "_Replanning helps with model errors_\n",
    "Errors can accumulate over time steps, this needs to be fixed at each error.  Your model can tell you how much to correct, you can replan your actions at each time steps as you find errors and then relearn the dynamics model over time.\n",
    "It is simple and solves the distribution mismatch but the open loop plan can perform poorly in stochastic domains.\n",
    "\n",
    "Version 1.5: iteratively collet data using MPC (replan at each step)\n",
    "It is robust where there are small errors, but it is computationally expensive and requires a planning algorithm.\n",
    "\n",
    "Version 2.0: backpropagate directly into policy\n",
    "Computationally cheap at runtime, but can be unstable, especially in stochastic domains\n",
    "\n",
    "#### What kind of models can we use?\n",
    "__Gaussian process__\n",
    "Fit a gp where the tuple is state and action and output is next state.\n",
    "They are very data-efficient but not great with non-smooth dynamics, they get slow when the dataset is large.\n",
    "\n",
    "__Neural Network__\n",
    "Input is a tuple of state and action output is next state.\n",
    "Euclidean training loss corresponds to Gaussians, more complex losses.  \n",
    "Very expressive and can use lots of data but are not great with low data regimes.\n",
    "\n",
    "__Other__\n",
    "Gaussian Mixture Model (GMM)\n",
    "Domain specific models\n",
    "Conditional Restricted Boltzmann Machines\n",
    "GPs and GPLVMs\n",
    "Linear and switching linear dynamical systems\n",
    "\n",
    "#### Global models and local models\n",
    "The model based RL will seek out the regions where the model is overly optimistic and you visit all the possible mistakes until you get the right behavior.\n",
    "Must find all the very good models for the state space to converge on a good solution.\n",
    "For easy tasks the model is much more complex than the policy. - instead of fitting a global model, fit a local model in the region you are located.\n",
    "\n",
    "#### Learning with local models and trust regions\n",
    "Local models- calculate new model at each time step.\n",
    "\n",
    "What controller to execute?\n",
    "Version 0.5: doesn't correct for drift or deviations.\n",
    "Version 1.0: gives you one trajectory over and over again.\n",
    "Version 2.0: stochastic, adds noise to vary the states.  How much noise depends on the importance of the step.\n",
    "\n",
    "__local model__\n",
    "- run controllers\n",
    "- collect dynamics\n",
    "- fit dynamics to get derivatives\n",
    "- feed derivatives them to LQR, repeat\n",
    "- fit the dynamics\n",
    "\n",
    "Can we do better?\n",
    "Version 2.0 using Bayesian linear regression using a global model (GP, deep net, GMM)\n",
    "\n",
    "What if we go to far?\n",
    "The linearization is only valid in a local region. We want to keep the error of the controller as small as possible.  \n",
    "The contoller is a Linear Gaussian thing- creates trajectory over time.  We use a trust region.\n",
    "\n",
    "KL divergengces between trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Lecture 5\n",
    "\n",
    "---\n",
    "__[Berekely DeepRL Video Lecture 5](https://youtu.be/o0Ebur3aNMo)__\n",
    "\n",
    "__Gaze Heuristic__\n",
    "Useful when the physics of a task is prohibitively complex: view an object and make corrections as the object moves to keep it in the same location in the field of view.  Allows you to ignore the model of the environment when you develop a policy.\n",
    "\n",
    "##### Can we backpropogate into the policy?\n",
    "Problems: each action effects all future actions- so early action changes will have a big gradient.  If you change an action at the begining, it changes the last action A LOT.  (susceptible to vanishing/exploding gradients)<br>\n",
    "The dynamics aren't simple, they are chosen by nature and can be very complex.<br>\n",
    "\n",
    "##### Collocation Methods\n",
    "The early actions don't 'wave the end' around but are more expensive computationally.<br>\n",
    "Extending trajetory optimization to policy optimization.<br>\n",
    "\n",
    "How to solve the constraing optimization: \n",
    "- dual gradient descent: imposes constraints, \n",
    "- DDG with small tweak- augmented Lagrangian, makes it more well behaved, keeps constraing violation small.\n",
    "\n",
    "#### Contraining trajectory optimization with dual gradient descent\n",
    "Minimize the cost of some trajectory, subject to a constraint that the actions along the trajectory are the actions the policy would have taken.<br>\n",
    "\n",
    "_Optimize to convergence_\n",
    "1. minimize in respect to trajectory \n",
    "2. minimize with respect to the policy parameters \n",
    "3. increment the dual variable\n",
    "\n",
    "__Guided Policy Search__ (deterministic policies, dynamics)\n",
    "- constrained trajectory optimization (generating the data for imitation)\n",
    "- imitation of an optimal control expert (imitation learning)\n",
    "- the teacher adapts to the learner to avoid actions the learner can't mimic.\n",
    "\n",
    "Need to choose:<br>\n",
    "form of p(tau)<br>\n",
    "optimization method for p(tau)<br>\n",
    "surogate cost (how to enforce constraint)<br>\n",
    "supevised objective for the policy<br>\n",
    "\n",
    "__Stochastic Case__ (gaussian distribution)\n",
    "1. Optimize p(tau) with respect to some surrogate cost. (LQR)\n",
    "2. Optimize theta to some supervised objective.\n",
    "3. Increment or modify the dual variables\n",
    "Can now be used with learned local models.  \n",
    "\n",
    "#### Input Remapping Trick\n",
    "Assumes that you have the state. (observations are not markovian, but states are)<br>\n",
    "At training time you have a full state to do the trajectory optimization but for the policy you only give it the observations so that at test time, it can work with observation input.\n",
    "\n",
    "### Case Study: Vision-based control with GPS\n",
    "Paper: End-to-End Training of Deep Visuomotor Policies<br>\n",
    "Controlling a one arm robot using vision- modifying trajectorys to best suit policy optimization proceedure.<br>\n",
    "\n",
    "---\n",
    "Paper: Deep Learning for Real-Time Atari Game play Using Offline Monte-Carlo Tree Search Planning <br>\n",
    "Imitation learning with DAgger, learn to play atari games without Q-learning, instead model based policy.\n",
    "\n",
    "*DAgger Problem*<br>\n",
    "Computer can label the action, but policy must be run in the real world.  Initially the policy won't be very good, sometimes you want to aviod running bad policies (safety...).<br>\n",
    "Model Predictive Control: PLATO algorithm<br>\n",
    "Use this to avoid running a partially trained policy.  Pi hat minimizes the modified cost function.  Take actions that are as close to possible, but minimize the cost.\n",
    "\n",
    "replanning = Model Predictive Control (MPC)\n",
    "\n",
    "### DAgger vs Guided Policy Search (GPS)\n",
    "- DAgger does not require an adaptive expert, any expert will work as long as states can be labeled from learned policy.  Expert's behavior is able to achieve a bounded loss- can learn something from the expert and measure how wrong you are.\n",
    "- GPS adapts the expert behavior, no need for bounded loss because expert changes (NPC).  \n",
    "\n",
    "#### Why do we want to imitate optimal control?\n",
    "These techniques are stable and easy to use.  Once you have the planner, the rest is supervised learning.  <br>\n",
    "Input mapping trick can allow for quicly getting the policy.<br>\n",
    "Overcomes the challenges of backpropogating into a policy.<br>\n",
    "Viable for real systems, efficient.<br>\n",
    "\n",
    "**Limitations**\n",
    "You need a model.<br>\n",
    "It takes time and data to learn the model<br>\n",
    "There are assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
