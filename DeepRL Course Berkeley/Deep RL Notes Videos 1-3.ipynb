{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berkeley Deep RL Course Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Lecture 1\n",
    "***\n",
    "__[Berkeley DeepRL Course Lecture 1 Video](https://youtu.be/8jQIKgTzQd4)__\n",
    "\n",
    "Reinforcement learning has a stateful world.  The input is not just randomly sampled, it depends on the previous inputs and previous actions.  \n",
    "\n",
    "The input data changes as a function of what it is doing: takes an action and receives a cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Lecture 2\n",
    "***\n",
    "__[Berkeley DeepRL Course Lecture 2 Video](https://youtu.be/kl_G95uKTHw)__\n",
    "\n",
    "_Definitions_\n",
    "\n",
    "State: $X_{t}$,<br>\n",
    "Observation: $O_{t}$,<br>\n",
    "Action: $U_{t}$, <br>\n",
    "Policy: $\\pi_{\\theta}(O_{t}|U_{t})$,<br>\n",
    "Cost Function: $C(O_{t}|U_{t})$,<br>\n",
    "Reward Function: Policy: $R(O_{t}|U_{t})$, (just negative C)<br>\n",
    "\n",
    "### Imitation Learning and Behavior Cloning\n",
    "\n",
    "Sequential learning will have time steps. Can have discrete or continuous actions. \n",
    "\n",
    "Markov Property: each x influences the next one, if you know the current state you can forget about the past (it is included in the current state).\n",
    "\n",
    "State X has to satisfy the Markov property, Observation O does not.  (process is partial Markov Model)\n",
    "If fully observed O and X are the same- then full Markov Model.\n",
    "\n",
    "#### Imitation Learning\n",
    "\n",
    "observations --> training data --> supervised learning --> output  (does this work: no, but...)\n",
    "\n",
    "__Distribution Mismatch Problem__\n",
    "Every time you do training with supervised learning and then run policy- it will deviate a little from the training trajectory (epsilon error).  The error causes a small error in the state- leading to a state that was not present in the training set. The next step will add more error, increasingly compounding as the trajectory progresses.\n",
    "\n",
    "The problem can be addressed by adding in self-stabilizing methods to estimate and correct for the error.  A distribution of training data, rather than just a path makes it more stable.  Include data that has errors/mistakes to correct and conduct a trajectory distribution to keep the error from compounding. \n",
    "\n",
    "Instead of trying to be creative with the policy, be creative about the data.\n",
    "\n",
    "The cost function for imitation learning is just the distance between the action versus the human action.\n",
    "\n",
    "##### DAgger: Dataset Aggregation\n",
    "Collect training data from the policy- but how to get the labels?\n",
    "\n",
    "How it works:\n",
    "1. Collect human data, train using supervised learning algorithm.\n",
    "2. Run the policy to collect a new set of observations. (can be a mixture)\n",
    "3. Ask a human to label the new observations with what the human thinks is a good action.\n",
    "4. Construct a dataset that is a union of the original and new datasets.\n",
    "5. Repeat until the dataset becomes primarily from the policy.\n",
    "\n",
    "Problems with DAgger: \n",
    "-step 3 is not intuitive to implement (like a video) and non-trivial.\n",
    "\n",
    "Imitation learning is often insufficient by itself.  I can work well with some hacky modifications, samples from a stable trajectory data, and adding more on-policy data (DAgger).\n",
    "\n",
    "##### Problems with Imitation Data\n",
    "- Humans need to provide the data- this is finite and may not be sufficient.\n",
    "- There are actions that humans are not good at and can't demonstrate well, some are impossible.\n",
    "- Humans can learn autonomously- this gives unlimited data collection, can constantly improve (machines can't through imitation learning)\n",
    "\n",
    "__Learning without humans?__\n",
    "What makes an action good? minimize the sum of the cost over all time steps.\n",
    "\n",
    "__Cost and Reward Function Problems__\n",
    "Reward functions are not obvious, can be hard to evaluate (is there water in the class... hard to answer with a computer).\n",
    "\n",
    "\n",
    "\n",
    "#### Research Papers on Imitation learning\n",
    "\n",
    "- A Machine learning Approach to Visual Perception of Forest Trails for Mobile Robots\n",
    "Goal: to navigate a quadcopter down a forest trail without it going off the trail.  \n",
    "Abstracted the scene to forward, left, right.\n",
    "Perceiving the images is difficult: imitation learning.\n",
    "Collected training data by having a person wear a headband with 3 cameras (left, right, center) walk through the path.\n",
    "Used a classifier, then tested with a person using a cell phone camera.\n",
    "\n",
    "- Learning Transferable Policies for Monocular Reactive MAV Control\n",
    "Goal: use human labeled data from quadcopter to train copter to fly in winter (data from summer).\n",
    "Trained a CNN to map from the image to the actions.  \n",
    "Train a network to get upper levels from winter/spring to match.\n",
    "Attempted to transfer from one type of drone to another, one type of forest to another, one forest in summer to one in the winter.\n",
    "\n",
    "- Learning Real Manipulation Tasks from Virtual demonstrations using LSTM\n",
    "Goal: put an object on the shelf, push an object.\n",
    "Not using computer vision, just using tags.\n",
    "If the grip fails, it will try again.\n",
    "\n",
    "#### Other Topics in Imitation Learning\n",
    "- Structured Prediction: network that answers questions.\n",
    "- Interaction and Active Learning: reason about which states to visit to best learn.\n",
    "- Inverse Reinforcement learning: instead of copying, figure out what was the goal and determine on your own how to accomplish it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Lecture 3\n",
    "\n",
    "---\n",
    "__[Berkeley DeepRL Course Lecture 3 Video](https://youtu.be/mZtlW_xtarI)__\n",
    "\n",
    "#### Making Decisions under Known Dynamics\n",
    "There is a sequence to the cost- you could choose a low cost action at the beginning that leads to very high costs later.\n",
    "\n",
    "#### Trajectory Optimization: backpropagation through dynamical systems\n",
    "![Trajectory Formula](Trajectory-Formula.jpg)\n",
    "Boils down to: differentiate via back propagation and optimize. (it helps to use 2nd order methods)\n",
    "\n",
    "#### Linear Dynamics:  linear-quadratic regulator (LQR)\n",
    "_Shooting Methods_\n",
    "Optimizing over actions treating the states as constants.  The action at step one affects the whole trajectory.  Simple.\n",
    "\n",
    "_Collocation_\n",
    "Keep the original constrained optimization problem that includes both the actions and states.  \n",
    "\n",
    "_Linear Case (a shooting method) LQR_\n",
    "\n",
    "Simplified version for linear dynamical systems (robotics).  Make a simplified assumption about the dynamics of the system, assume the dynamics (f) is linear.  The next state can be expressed as a linear equation of the previous state.\n",
    "Assume the cost is a quadratic function of the state. Start at the end of the trajectory, go backwards through time and compute all the actions in between (backward recursion for actions, then forward recursion to get the values).\n",
    "\n",
    "#### Nonlinear Dynamics: differential dynamic programming (DDP) & iterative LQR\n",
    "_Nonlinear Case: DPP (differential dynamic programming)/iterative LQR_\n",
    "\n",
    "Approximate a nonlinear system as a linear-quadratic system (uses Taylor expansion). \n",
    "_Iteratively optimizing trajectory_\n",
    "1. Current guess about states and actions \n",
    "2. Linearized dynamics around those states and actions \n",
    "3. Quadracize you costs around those states and actions\n",
    "4. Run LQR with a four pass using the true nonlinear dynamics\n",
    "5. Get updated actions\n",
    "6. Repeat until convergence\n",
    "\n",
    "Newton's method can allow large errors.\n",
    "\n",
    "##### Case Study: nonlinear model-predictive control\n",
    "_Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization_\n",
    "(no deep or reinforcement learning)\n",
    "Uses iterative LQR with model predictive control (NPC) where ever single time you step replan the optimal trajectory.\n",
    "\n",
    "#### Discrete Systems: Monte-Carlo tree search (MCTS)\n",
    "Discrete systems (like Atari games)\n",
    "States (memory), observations (pixels), actions\n",
    "State and observations are often combined because they are essentially the same for these types of games. For this- we consider the image the state.  \n",
    "The image and actions are discrete.\n",
    "\n",
    "_Monte Carlo tree search_ heuristic version of a search algorithm, frame a planning problem as a search problem\n",
    "\n",
    "Start at time step, apply actions- then states.  Can ignore some of the states off by making an estimate of how good the state is, the policy may be random. \n",
    "Choose paths to search first by starting at leaves and expanding better subtrees first.  Choose the nodes that appear to be best so far with a preference nodes that haven't been visited often.  The system is more confident in the score of the commonly visited nodes and will be encouraged to explore less visited ones to improve the accuracy of the score.\n",
    "\n",
    "_Generic MCTS sketch_\n",
    ">find a leaf st using TreePolicy(s1)\n",
    ">evaluate the def using DefaultPolicy(st)\n",
    ">update all valus in tree between st and s1\n",
    ">repeat\n",
    ">take best action from s1\n",
    "\n",
    "_UCT TreePolicy(st)_\n",
    ">if st is not fully expanded, choose new at (child)\n",
    ">else choose child with best Score(st+1) \n",
    "\n",
    "What is wrong with known dynamics?\n",
    "There are domains and physics where the dynamics are exceptionally complex."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
