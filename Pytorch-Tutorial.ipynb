{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Pytorch Tutorial\n",
    "__[Towards Data Science Tutorial](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Regression Problem\n",
    "Linear regression with a single feature x.<br>\n",
    "Steps:\n",
    "1. generate data, label, and add noise\n",
    "2. split the data into training and validation 80/20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "#shuffle indexes\n",
    "index = np.arange(100)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "# get first 80 values for training\n",
    "train_index = index[:80]\n",
    "\n",
    "# get last 20 for validation\n",
    "validate_index = index[80:]\n",
    "\n",
    "x_train, y_train = x[train_index], y[train_index]\n",
    "x_validate, y_validate = x[validate_index], y[validate_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print plots of the data\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Create Figure and Subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4), sharey=False, dpi=120)\n",
    "\n",
    "# Plot\n",
    "ax1.plot(x_train, y_train, 'g.')  # green dots\n",
    "ax2.plot(x_validate, y_validate, 'b.')  # blue dots\n",
    "\n",
    "# Title, X and Y labels, X and Y Lim\n",
    "ax1.set(title='Generated Data: Train', xlabel='x axis', ylabel='y axis')\n",
    "ax2.set(title='Generated Data: Validate', xlabel='x axis', ylabel='y axis')\n",
    "# ax2.yaxis.set_ticks_position('none') \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "* Compute the loss\n",
    "    * regression uses Mean Squared Error (MSE)\n",
    "    * batch gradient descent uses all the training points\n",
    "    * stochastic gradient descent uses one point in time\n",
    "    * any other size is mini-batch gradient descent\n",
    "* Compute the gradients\n",
    "    * how much does MSE loss change when we vary a parameter?\n",
    "    * use partial derivatives of each parameter\n",
    "* Update the parameters\n",
    "    * use the gradients to update the parameters \n",
    "    * reverse the sign to minimize loss (rather than maximize)\n",
    "    * learning rate: multiplicative factor to apply to the gradient for the update\n",
    "* Repeat for n epochs..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Linear Regression in Numpy\n",
    "Training:\n",
    "* random initiation of parameters/weights\n",
    "* initialization of hyperparameters\n",
    "\n",
    "Use random seed for reproducibililty<br>\n",
    "\n",
    "__Training steps for each epoch:__<br>\n",
    "\n",
    "1. compute model predictions\n",
    "2. compute the loss\n",
    "3. compute the gradients\n",
    "4. updated the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a and b after initialization:\n",
      "[0.49671415] [-0.1382643]\n",
      "a and b after linear regression:\n",
      "[1.02354094] [1.96896411]\n"
     ]
    }
   ],
   "source": [
    "# Initializes parameters \"a\" and \"b\" randomly\n",
    "np.random.seed(42) # use seed for reproducibility\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "print('a and b after initialization:')\n",
    "print(a,b)\n",
    "\n",
    "# set the learning rate (choosing rate not in this tutorial)\n",
    "lr = 1e-1\n",
    "\n",
    "# set the number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # compute the predicted output of the model\n",
    "    yhat = a + b * x_train\n",
    "    \n",
    "    # get the error (using batch gradient descent)\n",
    "    error = (y_train - yhat)\n",
    "    \n",
    "    # compute the mean squared error\n",
    "    loss = (error**2).mean()\n",
    "    \n",
    "    # compute the gradients for a and b\n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    # update the parameters using gradients and learning rate\n",
    "    a = a - lr * a_grad\n",
    "    b = b - lr * b_grad\n",
    "    \n",
    "print('a and b after linear regression:')\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a and b scikit linear regression:\n",
      "[1.02354075] [1.96896447]\n"
     ]
    }
   ],
   "source": [
    "# use skikit-learn to check/ verify it is correct\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print('a and b scikit linear regression:')\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a tensor?\n",
    "- scalar: no dimensions\n",
    "- vector: one dimension\n",
    "- matrix: two dimensions\n",
    "- tensor: more than 2 dimensions<br>\n",
    "\n",
    "Can refer to vectors/matrices as tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data, Devices and CUDA\n",
    "- can convert from a numpy array to a torch tensor\n",
    "- can send the tensor to the gpu or whatever you specify\n",
    "\n",
    "You cannot use cuda in a virutal environment without hacky mods.  Will not implement here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "# uses cuda if available, if not - uses cpu.  gpu not avaibable in vrtual box.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# convert numpy arrays to torch tensor\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "# you cannot turn a gpu tensor back into a numpy array without moving\n",
    "# it to the cpu first with cpu()\n",
    "\n",
    "# check werhere the array/tensors are stored (cpu vs gpu)\n",
    "# use type() or .type()\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch and assign to device\n",
    "You can create a cpu torch and assign it to the gpu later.  But it is better practice to create and assign the torch\n",
    "at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#  create parameter a and b and use REQUIRES_GRAD=TRUE to apply gradient descent\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# use the device argument at creation to set the device\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "Autograd is the automatic differentiation tool in Pytorch. <br>\n",
    "torch.autograd.backward() computes the sm of the gradients of given tensors w.r.t graph leaves<br>\n",
    "each time the gradients are used to update the parameters, they need to be zeroed out with zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#  use backward() and zero_() methods to update the gradients\n",
    "\n",
    "# parameters from above, copied here for reference\n",
    "# lr = 1e-1\n",
    "# n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # use loss.backward() to get gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # need to use no_grad to keep the update (b/c dynamic computation graph)\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"172pt\" height=\"171pt\"\n",
       " viewBox=\"0.00 0.00 171.50 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-167 167.5,-167 167.5,4 -4,4\"/>\n",
       "<!-- 139824885355912 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139824885355912</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"118,-21 26,-21 26,0 118,0 118,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139824885355072 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139824885355072</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-92 0,-92 0,-57 54,-57 54,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139824885355072&#45;&gt;139824885355912 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139824885355072&#45;&gt;139824885355912</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-56.6724C45.4798,-48.2176 52.5878,-38.1085 58.6352,-29.5078\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-31.4169 64.4601,-21.2234 55.8452,-27.3906 61.5714,-31.4169\"/>\n",
       "</g>\n",
       "<!-- 139824885354736 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139824885354736</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-85 72.5,-85 72.5,-64 163.5,-64 163.5,-85\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-71.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139824885354736&#45;&gt;139824885355912 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139824885354736&#45;&gt;139824885355912</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-63.9317C103.7191,-54.6309 93.821,-40.8597 85.7479,-29.6276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-27.3753 79.761,-21.2979 82.7553,-31.4608 88.4395,-27.3753\"/>\n",
       "</g>\n",
       "<!-- 139824885354680 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139824885354680</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-163 91,-163 91,-128 145,-128 145,-163\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139824885354680&#45;&gt;139824885354736 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139824885354680&#45;&gt;139824885354736</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M118,-127.9494C118,-118.058 118,-105.6435 118,-95.2693\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-95.0288 118,-85.0288 114.5001,-95.0289 121.5001,-95.0288\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f2b41a97f60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(yhat)\n",
    "# blue boxes: tensors use as parameters, the ones to compute gradients for\n",
    "# gray box:   python operation that involves gradient computing tensor or its dependencies\n",
    "# green box:  same as gray, but the starting point for the computation of gradients\n",
    "\n",
    "# anything not used in computing gradients is not show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Parameters\n",
    "Pytorch has optimezers like SGD (stochatic gradient descent) and Adam.<br>\n",
    "The optimizer uses the parameters we want to update, the learning rate (and other hyperparameters) and updates using the  step() method.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# from above, reseting all the variables...\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# set up the optimizer\n",
    "optimizer = optim.SGD([a,b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # no more manually calculating the gradients\n",
    "    loss.backward()    \n",
    "    \n",
    "    # no more manual updates\n",
    "    optimizer.step()\n",
    "    \n",
    "    # use the optimizer to let the gradients go...\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Computation\n",
    "This doesn't need to be done manually. <br>\n",
    "MSE is used for linear regression, there are many others implemented __[See Documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)__.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# from above, reseting all the variables...\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# set up the optimizer\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    \n",
    "    # No more manual loss\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "\n",
    "    # no more manually calculating the gradients\n",
    "    loss.backward()    \n",
    "    \n",
    "    # no more manual updates\n",
    "    optimizer.step()\n",
    "    \n",
    "    # use the optimizer to let the gradients go...\n",
    "    optimizer.zero_grad()\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model Class\n",
    "\n",
    "The model class is a python class that inherits from module.\n",
    "\n",
    "Must implement:\n",
    "\n",
    "- \\__init__(self):  defines the parts of the model (parameters)<br>\n",
    "- forward(self,x):  preforms the computation, it outputs a prediction given the input x\n",
    "\n",
    "The whole model should be called, not forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# using the nn.Module to create a linear regression.\n",
    "\n",
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        # this tells pytorch that these tensors should be part of the parameters for the model\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n",
      "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "# call the model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Now we can create a model and send it at once to the device\n",
    "# if the parameters are in the gpu, then the model must be in the gpu too\n",
    "model = ManualLinearRegression().to(device)\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # puts the model in training mode, does not perform a training step\n",
    "    # some models behave differently in trainging/testing (ie Dropout)\n",
    "    model.train()\n",
    "\n",
    "    # replaces the manual training\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " # nested linear model\n",
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear layer with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call to the layer to make predictions\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential models\n",
    "\n",
    "The output of one layer is fed into the next.\n",
    "\n",
    "### Training Step\n",
    "Neaten up the code to perform a training step:<br>\n",
    "Function that takes the optimizer, loss, and model, then returns another function that performs a training step.<br>\n",
    "The function takes features and labels as arguments and returns loss.<br>\n",
    "The function can be called from our training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "losses = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)\n",
    "    \n",
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset\n",
    "\n",
    "Implement the Dataset class: a list of tuples (features, labels)<br>\n",
    "- \\__init__(self): takes in arguments needed to build a list of tuples (can be csv, two tensors...)\n",
    "- \\__get_item__(self, index): indexes the dataset and returns a tuple.  Can be used to load data items on demand if the set is large.\n",
    "-  \\__len__(self): returns the size of the whole dataset.\n",
    "\n",
    "### PytTorch DataLoader\n",
    "\n",
    "PyTorch DataLoader can slice our dataset into mini-batches. <br>\n",
    "It will act like an iterator and can shuffle or not, and fetch a different batch each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713]), tensor([2.4745]))\n",
      "(tensor([0.7713]), tensor([2.4745]))\n"
     ]
    }
   ],
   "source": [
    "# a custom dataset that takes two tensors as arguments (features, labels)\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "class CustomDataset(Dataset):  # inherits the dataset class\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "# Dont want to use gpu tensors because the can eat up the ram in the gpu\n",
    "# we don't want to load an entire dataset into the gpu...\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])\n",
    "\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#set up the dataloader\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([1.0249])), ('b', tensor([1.9695]))])\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random split\n",
    "Performs a training-validation split- must be applied to the whole dataset.<br>\n",
    "Then build a DataLoader for each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Add the evaluation of the model in the training loop (validation loss)<br>\n",
    "Add another inner loop to handle the mini-batches from the validation loader.<br>\n",
    "Make predictions about the model.<br>\n",
    "Compute the loss.<br>\n",
    "\n",
    "---\n",
    "Gradients belong in training, not in validation.<br>\n",
    "eval() puts the modelin evaluation. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([1.0340])), ('b', tensor([1.9253]))])\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
