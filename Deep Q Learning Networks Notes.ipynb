{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "---\n",
    "Learn more about the math __[Lecture 14 | Deep Reinforcement Learning (Stanford)](http://goo.gl/hTj627)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/OYhFoMySoVs\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x13d328265f8>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src=\"https://www.youtube.com/embed/OYhFoMySoVs\", width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque  # list accessible from both ends\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense  # only using dense layers\n",
    "from keras. optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment for deep rl\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the state size\n",
    "state_size = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the action space size\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the batch size for the gradient descent\n",
    "batch_size = 32  # can be varied by power of 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes is the number games to play \n",
    "# provides more data for training\n",
    "# each episode data is collected to train the deep rl\n",
    "n_episodes = 1001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an output directory for model\n",
    "output_dir = 'model_output/cartpole'\n",
    "# create directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formally Define the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        #inital parameters\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # define the memory- how many state/action pairs to remember\n",
    "        # limits the amount of information, gives us a sample from all around data set\n",
    "        # get a better diversity from the data, using a deque to keep only some \n",
    "        # of the samples, once the limit is reached, older ones are discarded.\n",
    "        \n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        \n",
    "        # set hyper parameters\n",
    "        # gamma is how much discount to apply\n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        \n",
    "        # epsilon is the exploration rate for the agent\n",
    "        # two modes the action can take: best possible based on what is learned (exploitation), \n",
    "        # or random new actions (exploration).  Exploration is helpful because domain is so large\n",
    "        # it would be too time consuming to try everything but exploitation might direct the actions \n",
    "        # away from a more optimal solution over time.\n",
    "        \n",
    "        self.epsilon = 1.0 # skewed 100% towards exploration (at the begining because it doesn't know anything)\n",
    "        \n",
    "        # decay epislon over time to shift from exploring at random to exploiting \n",
    "        # the information that is now known.  The floor is how low the epsilon can go- even when\n",
    "        # we have decayed to zero, so a little exploration will always occur. \n",
    "        \n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        # learning rate stochastic gradient learning rate\n",
    "        \n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        #private method for build model\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\" Define the dense neural network for approximating the Q* \"\"\"\n",
    "        \n",
    "        #  set up a keras model\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # create a shallow neural network\n",
    "        # use a dense network with 24 input neurons, input dimensions are num of states, use relu for activation. \n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))        \n",
    "        \n",
    "        # output has as many neurons as possible actions, linear activation to directly model actions \n",
    "        # don't want a probablity estimate\n",
    "        \n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        # for output use mean-squared-error for cost estimate (try cross-entropy, but it is not as good)\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\" \n",
    "            Appends the information into the memory deck\n",
    "            \n",
    "            Parameters:\n",
    "                state at current time step\n",
    "                action at current time step\n",
    "                reward at current time step\n",
    "                next_state\n",
    "                done    \n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "            Decide what action to take within a state: explore/exploit\n",
    "            Parameters:\n",
    "                state\n",
    "        \"\"\"\n",
    "        \n",
    "        # sample a random value between 0 and 1, if less than or equal to epsilon, act randomly (exploration)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # to exploit, use theta weights and predict method on the model so there is a guess on the \n",
    "        # next best action\n",
    "        \n",
    "        act_values = self.model.predict(state)\n",
    "        \n",
    "        # return the best choice\n",
    "        \n",
    "        return np.argmax(act_values[0])\n",
    "        \n",
    "    def replay(self, batch_size):\n",
    "    \n",
    "        # create a mini batch that is a random sample from the deck of memories\n",
    "        # the batch size is the number of memories\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # if the max time steps allowed is reached, or the game ended then the done\n",
    "        # target is = reward (we know how the game ends)\n",
    "        \n",
    "        for (state, action, reward, next_state, done) in minibatch:\n",
    "            target = reward\n",
    "        \n",
    "        # if not done, then future reward must be estimated (current reward plus the discounted future reward.)\n",
    "        \n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "                \n",
    "        # map the maximized future reward to the current reward using theta\n",
    "        \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "                          \n",
    "        # fit a model to train using current state, predicted future reward, for a single epoch\n",
    "                          \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        # decrease epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "         \n",
    "    def load(self, name):\n",
    "        \"\"\" loading weights \"\"\"\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "    def save(self, name):\n",
    "        \"\"\" savin weights for later use \"\"\"\n",
    "        self.model.save_weights(name)\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/1001, score: 134, e: 0.043\n",
      "Episode: 1/1001, score: 137, e: 0.042\n",
      "Episode: 2/1001, score: 165, e: 0.042\n",
      "Episode: 3/1001, score: 112, e: 0.042\n",
      "Episode: 4/1001, score: 122, e: 0.042\n",
      "Episode: 5/1001, score: 128, e: 0.041\n",
      "Episode: 6/1001, score: 110, e: 0.041\n",
      "Episode: 7/1001, score: 199, e: 0.041\n",
      "Episode: 8/1001, score: 40, e: 0.041\n",
      "Episode: 9/1001, score: 104, e: 0.041\n",
      "Episode: 10/1001, score: 17, e: 0.04\n",
      "Episode: 11/1001, score: 37, e: 0.04\n",
      "Episode: 12/1001, score: 30, e: 0.04\n",
      "Episode: 13/1001, score: 28, e: 0.04\n",
      "Episode: 14/1001, score: 26, e: 0.04\n",
      "Episode: 15/1001, score: 10, e: 0.039\n",
      "Episode: 16/1001, score: 13, e: 0.039\n",
      "Episode: 17/1001, score: 15, e: 0.039\n",
      "Episode: 18/1001, score: 16, e: 0.039\n",
      "Episode: 19/1001, score: 16, e: 0.039\n",
      "Episode: 20/1001, score: 13, e: 0.038\n",
      "Episode: 21/1001, score: 15, e: 0.038\n",
      "Episode: 22/1001, score: 19, e: 0.038\n",
      "Episode: 23/1001, score: 17, e: 0.038\n",
      "Episode: 24/1001, score: 21, e: 0.038\n",
      "Episode: 25/1001, score: 20, e: 0.038\n",
      "Episode: 26/1001, score: 15, e: 0.037\n",
      "Episode: 27/1001, score: 14, e: 0.037\n",
      "Episode: 28/1001, score: 15, e: 0.037\n",
      "Episode: 29/1001, score: 10, e: 0.037\n",
      "Episode: 30/1001, score: 11, e: 0.037\n",
      "Episode: 31/1001, score: 14, e: 0.036\n",
      "Episode: 32/1001, score: 19, e: 0.036\n",
      "Episode: 33/1001, score: 22, e: 0.036\n",
      "Episode: 34/1001, score: 44, e: 0.036\n",
      "Episode: 35/1001, score: 199, e: 0.036\n",
      "Episode: 36/1001, score: 53, e: 0.035\n",
      "Episode: 37/1001, score: 199, e: 0.035\n",
      "Episode: 38/1001, score: 54, e: 0.035\n",
      "Episode: 39/1001, score: 119, e: 0.035\n",
      "Episode: 40/1001, score: 54, e: 0.035\n",
      "Episode: 41/1001, score: 29, e: 0.035\n",
      "Episode: 42/1001, score: 13, e: 0.034\n",
      "Episode: 43/1001, score: 32, e: 0.034\n",
      "Episode: 44/1001, score: 48, e: 0.034\n",
      "Episode: 45/1001, score: 69, e: 0.034\n",
      "Episode: 46/1001, score: 78, e: 0.034\n",
      "Episode: 47/1001, score: 46, e: 0.034\n",
      "Episode: 48/1001, score: 199, e: 0.033\n",
      "Episode: 49/1001, score: 16, e: 0.033\n",
      "Episode: 50/1001, score: 10, e: 0.033\n",
      "Episode: 51/1001, score: 14, e: 0.033\n",
      "Episode: 52/1001, score: 13, e: 0.033\n",
      "Episode: 53/1001, score: 15, e: 0.033\n",
      "Episode: 54/1001, score: 17, e: 0.032\n",
      "Episode: 55/1001, score: 23, e: 0.032\n",
      "Episode: 56/1001, score: 32, e: 0.032\n",
      "Episode: 57/1001, score: 20, e: 0.032\n",
      "Episode: 58/1001, score: 70, e: 0.032\n",
      "Episode: 59/1001, score: 98, e: 0.032\n",
      "Episode: 60/1001, score: 118, e: 0.031\n",
      "Episode: 61/1001, score: 118, e: 0.031\n",
      "Episode: 62/1001, score: 161, e: 0.031\n",
      "Episode: 63/1001, score: 74, e: 0.031\n",
      "Episode: 64/1001, score: 83, e: 0.031\n",
      "Episode: 65/1001, score: 199, e: 0.031\n",
      "Episode: 66/1001, score: 199, e: 0.031\n",
      "Episode: 67/1001, score: 199, e: 0.03\n",
      "Episode: 68/1001, score: 199, e: 0.03\n",
      "Episode: 69/1001, score: 199, e: 0.03\n",
      "Episode: 70/1001, score: 199, e: 0.03\n",
      "Episode: 71/1001, score: 78, e: 0.03\n",
      "Episode: 72/1001, score: 55, e: 0.03\n",
      "Episode: 73/1001, score: 109, e: 0.029\n",
      "Episode: 74/1001, score: 76, e: 0.029\n",
      "Episode: 75/1001, score: 82, e: 0.029\n",
      "Episode: 76/1001, score: 50, e: 0.029\n",
      "Episode: 77/1001, score: 199, e: 0.029\n",
      "Episode: 78/1001, score: 175, e: 0.029\n",
      "Episode: 79/1001, score: 192, e: 0.029\n",
      "Episode: 80/1001, score: 199, e: 0.028\n",
      "Episode: 81/1001, score: 136, e: 0.028\n",
      "Episode: 82/1001, score: 82, e: 0.028\n",
      "Episode: 83/1001, score: 36, e: 0.028\n",
      "Episode: 84/1001, score: 15, e: 0.028\n",
      "Episode: 85/1001, score: 26, e: 0.028\n",
      "Episode: 86/1001, score: 199, e: 0.028\n",
      "Episode: 87/1001, score: 155, e: 0.027\n",
      "Episode: 88/1001, score: 126, e: 0.027\n",
      "Episode: 89/1001, score: 185, e: 0.027\n",
      "Episode: 90/1001, score: 30, e: 0.027\n",
      "Episode: 91/1001, score: 57, e: 0.027\n",
      "Episode: 92/1001, score: 147, e: 0.027\n",
      "Episode: 93/1001, score: 42, e: 0.027\n",
      "Episode: 94/1001, score: 43, e: 0.027\n",
      "Episode: 95/1001, score: 37, e: 0.026\n",
      "Episode: 96/1001, score: 42, e: 0.026\n",
      "Episode: 97/1001, score: 45, e: 0.026\n",
      "Episode: 98/1001, score: 127, e: 0.026\n",
      "Episode: 99/1001, score: 59, e: 0.026\n",
      "Episode: 100/1001, score: 199, e: 0.026\n",
      "Episode: 101/1001, score: 109, e: 0.026\n",
      "Episode: 102/1001, score: 105, e: 0.025\n",
      "Episode: 103/1001, score: 185, e: 0.025\n",
      "Episode: 104/1001, score: 165, e: 0.025\n",
      "Episode: 105/1001, score: 47, e: 0.025\n",
      "Episode: 106/1001, score: 102, e: 0.025\n",
      "Episode: 107/1001, score: 103, e: 0.025\n",
      "Episode: 108/1001, score: 115, e: 0.025\n",
      "Episode: 109/1001, score: 29, e: 0.025\n",
      "Episode: 110/1001, score: 199, e: 0.024\n",
      "Episode: 111/1001, score: 68, e: 0.024\n",
      "Episode: 112/1001, score: 199, e: 0.024\n",
      "Episode: 113/1001, score: 92, e: 0.024\n",
      "Episode: 114/1001, score: 133, e: 0.024\n",
      "Episode: 115/1001, score: 157, e: 0.024\n",
      "Episode: 116/1001, score: 168, e: 0.024\n",
      "Episode: 117/1001, score: 128, e: 0.024\n",
      "Episode: 118/1001, score: 122, e: 0.024\n",
      "Episode: 119/1001, score: 199, e: 0.023\n",
      "Episode: 120/1001, score: 157, e: 0.023\n",
      "Episode: 121/1001, score: 129, e: 0.023\n",
      "Episode: 122/1001, score: 145, e: 0.023\n",
      "Episode: 123/1001, score: 36, e: 0.023\n",
      "Episode: 124/1001, score: 98, e: 0.023\n",
      "Episode: 125/1001, score: 142, e: 0.023\n",
      "Episode: 126/1001, score: 199, e: 0.023\n",
      "Episode: 127/1001, score: 136, e: 0.022\n",
      "Episode: 128/1001, score: 121, e: 0.022\n",
      "Episode: 129/1001, score: 104, e: 0.022\n",
      "Episode: 130/1001, score: 108, e: 0.022\n",
      "Episode: 131/1001, score: 118, e: 0.022\n",
      "Episode: 132/1001, score: 105, e: 0.022\n",
      "Episode: 133/1001, score: 146, e: 0.022\n",
      "Episode: 134/1001, score: 54, e: 0.022\n",
      "Episode: 135/1001, score: 51, e: 0.022\n",
      "Episode: 136/1001, score: 42, e: 0.022\n",
      "Episode: 137/1001, score: 68, e: 0.021\n",
      "Episode: 138/1001, score: 94, e: 0.021\n",
      "Episode: 139/1001, score: 48, e: 0.021\n",
      "Episode: 140/1001, score: 64, e: 0.021\n",
      "Episode: 141/1001, score: 151, e: 0.021\n",
      "Episode: 142/1001, score: 64, e: 0.021\n",
      "Episode: 143/1001, score: 48, e: 0.021\n",
      "Episode: 144/1001, score: 48, e: 0.021\n",
      "Episode: 145/1001, score: 53, e: 0.021\n",
      "Episode: 146/1001, score: 115, e: 0.02\n",
      "Episode: 147/1001, score: 107, e: 0.02\n",
      "Episode: 148/1001, score: 121, e: 0.02\n",
      "Episode: 149/1001, score: 105, e: 0.02\n",
      "Episode: 150/1001, score: 151, e: 0.02\n",
      "Episode: 151/1001, score: 15, e: 0.02\n",
      "Episode: 152/1001, score: 46, e: 0.02\n",
      "Episode: 153/1001, score: 42, e: 0.02\n",
      "Episode: 154/1001, score: 59, e: 0.02\n",
      "Episode: 155/1001, score: 136, e: 0.02\n",
      "Episode: 156/1001, score: 147, e: 0.019\n",
      "Episode: 157/1001, score: 99, e: 0.019\n",
      "Episode: 158/1001, score: 111, e: 0.019\n",
      "Episode: 159/1001, score: 136, e: 0.019\n",
      "Episode: 160/1001, score: 140, e: 0.019\n",
      "Episode: 161/1001, score: 80, e: 0.019\n",
      "Episode: 162/1001, score: 110, e: 0.019\n",
      "Episode: 163/1001, score: 131, e: 0.019\n",
      "Episode: 164/1001, score: 34, e: 0.019\n",
      "Episode: 165/1001, score: 38, e: 0.019\n",
      "Episode: 166/1001, score: 199, e: 0.019\n",
      "Episode: 167/1001, score: 146, e: 0.018\n",
      "Episode: 168/1001, score: 199, e: 0.018\n",
      "Episode: 169/1001, score: 199, e: 0.018\n",
      "Episode: 170/1001, score: 182, e: 0.018\n",
      "Episode: 171/1001, score: 50, e: 0.018\n",
      "Episode: 172/1001, score: 127, e: 0.018\n",
      "Episode: 173/1001, score: 199, e: 0.018\n",
      "Episode: 174/1001, score: 199, e: 0.018\n",
      "Episode: 175/1001, score: 199, e: 0.018\n",
      "Episode: 176/1001, score: 178, e: 0.018\n",
      "Episode: 177/1001, score: 131, e: 0.018\n",
      "Episode: 178/1001, score: 81, e: 0.017\n",
      "Episode: 179/1001, score: 67, e: 0.017\n",
      "Episode: 180/1001, score: 199, e: 0.017\n",
      "Episode: 181/1001, score: 199, e: 0.017\n",
      "Episode: 182/1001, score: 45, e: 0.017\n",
      "Episode: 183/1001, score: 167, e: 0.017\n",
      "Episode: 184/1001, score: 199, e: 0.017\n",
      "Episode: 185/1001, score: 42, e: 0.017\n",
      "Episode: 186/1001, score: 41, e: 0.017\n",
      "Episode: 187/1001, score: 122, e: 0.017\n",
      "Episode: 188/1001, score: 199, e: 0.017\n",
      "Episode: 189/1001, score: 199, e: 0.016\n",
      "Episode: 190/1001, score: 191, e: 0.016\n",
      "Episode: 191/1001, score: 78, e: 0.016\n",
      "Episode: 192/1001, score: 199, e: 0.016\n",
      "Episode: 193/1001, score: 122, e: 0.016\n",
      "Episode: 194/1001, score: 137, e: 0.016\n",
      "Episode: 195/1001, score: 115, e: 0.016\n",
      "Episode: 196/1001, score: 199, e: 0.016\n",
      "Episode: 197/1001, score: 183, e: 0.016\n",
      "Episode: 198/1001, score: 32, e: 0.016\n",
      "Episode: 199/1001, score: 57, e: 0.016\n",
      "Episode: 200/1001, score: 166, e: 0.016\n",
      "Episode: 201/1001, score: 199, e: 0.016\n",
      "Episode: 202/1001, score: 199, e: 0.015\n",
      "Episode: 203/1001, score: 135, e: 0.015\n",
      "Episode: 204/1001, score: 48, e: 0.015\n",
      "Episode: 205/1001, score: 199, e: 0.015\n",
      "Episode: 206/1001, score: 199, e: 0.015\n",
      "Episode: 207/1001, score: 199, e: 0.015\n",
      "Episode: 208/1001, score: 194, e: 0.015\n",
      "Episode: 209/1001, score: 199, e: 0.015\n",
      "Episode: 210/1001, score: 140, e: 0.015\n",
      "Episode: 211/1001, score: 120, e: 0.015\n",
      "Episode: 212/1001, score: 116, e: 0.015\n",
      "Episode: 213/1001, score: 129, e: 0.015\n",
      "Episode: 214/1001, score: 132, e: 0.015\n",
      "Episode: 215/1001, score: 177, e: 0.014\n",
      "Episode: 216/1001, score: 167, e: 0.014\n",
      "Episode: 217/1001, score: 162, e: 0.014\n",
      "Episode: 218/1001, score: 172, e: 0.014\n",
      "Episode: 219/1001, score: 199, e: 0.014\n",
      "Episode: 220/1001, score: 199, e: 0.014\n",
      "Episode: 221/1001, score: 157, e: 0.014\n",
      "Episode: 222/1001, score: 199, e: 0.014\n",
      "Episode: 223/1001, score: 161, e: 0.014\n",
      "Episode: 224/1001, score: 199, e: 0.014\n",
      "Episode: 225/1001, score: 145, e: 0.014\n",
      "Episode: 226/1001, score: 142, e: 0.014\n",
      "Episode: 227/1001, score: 157, e: 0.014\n",
      "Episode: 228/1001, score: 177, e: 0.014\n",
      "Episode: 229/1001, score: 123, e: 0.013\n",
      "Episode: 230/1001, score: 166, e: 0.013\n",
      "Episode: 231/1001, score: 161, e: 0.013\n",
      "Episode: 232/1001, score: 199, e: 0.013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-b021a8017efd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# set the action state to the current state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\gym\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\gym\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\gym\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\gym\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\gym\\lib\\site-packages\\pyglet\\gl\\win32.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0m_gdi32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSwapBuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhdc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vsync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "done = False\n",
    "for e in range(n_episodes):\n",
    "    \n",
    "    # start each episode at the beginning \n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    # reshape the states (transpose) so they fit with the network\n",
    "    \n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "    # iterate over timesets of the game, set max number of timesteps it can run for\n",
    "    \n",
    "    for time in range(5000):\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        # set the action state to the current state\n",
    "        \n",
    "        action = agent.act(state) \n",
    "        \n",
    "        # get the information for the next step\n",
    "        \n",
    "        next_state, reward , done, _ = env.step(action)\n",
    "        \n",
    "        # calculate the reward (if done, -10)\n",
    "        \n",
    "        reward = reward if not done else -10\n",
    "        \n",
    "        # reshape next state\n",
    "        \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        \n",
    "        # remember the previous time steps\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # subsequent iteration state is the next state\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode: {}/{}, score: {}, e: {:.2}\".format(e, n_episodes, time, agent.epsilon))\n",
    "            break\n",
    "            \n",
    "    # train theta, give the agent a chance to update theta weights to improve\n",
    "    # gradient descent is about minimizing cost, we are maximizing reward- so technically ascent\n",
    "    \n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "        \n",
    "    # every 50 save the outputs \n",
    "    if e % 50 == 0:\n",
    "        agent.save(output_dir + \"weights_\" + '{:04d}'.format(e) +\".hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
