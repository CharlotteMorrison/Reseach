{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- __[Implementation from: Continuous Control with deep Reinforcement Learning](https://arxiv.org/pdf/1509.02971.pdf)__\n",
    "- __[Code and Tutorial From Paperspace blog](https://blog.paperspace.com/physics-control-tasks-with-deep-reinforcement-learning/)__\n",
    "- __[Array backed Sum Tree code](https://github.com/jaromiru/AI-blog/blob/master/SumTree.py)__<br>\n",
    "In reinforcement learning the agent is trained by interacting with the environment, not on a dataset, and learns by getting a reward for the interactions.\n",
    "- add in prioritize experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn #needed for building neural networks\n",
    "import torch.nn.functional as F #needed for activation functions\n",
    "import torch.optim as opt #needed for optimisation\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "print(\"Using torch version: {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE=1000000\n",
    "BATCH_SIZE=64\n",
    "GAMMA=0.99\n",
    "TAU=0.001       #Target Network HyperParameters Update rate\n",
    "LRA=0.0001      #LEARNING RATE ACTOR\n",
    "LRC=0.001       #LEARNING RATE CRITIC\n",
    "H1=400   #neurons of 1st layers\n",
    "H2=300   #neurons of 2nd layers\n",
    "\n",
    "MAX_EPISODES=5000 #number of episodes of the training\n",
    "MAX_STEPS=200    #max steps to finish an episode. An episode breaks early if some break conditions are met (like too much\n",
    "                  #amplitude of the joints angles or if a failure occurs). In the case of pendulum there is no break \n",
    "                #condition, hence no environment reset,  so we just put 1 step per episode. \n",
    "buffer_start = 100 #initial warmup without training\n",
    "epsilon = 1\n",
    "epsilon_decay = 1./100000 #this is ok for a simple task like inverted pendulum, but maybe this would be set to zero for more\n",
    "                     #complex tasks like Hopper; epsilon is a decay for the exploration and noise applied to the action is \n",
    "                     #weighted by this decay. In more complex tasks we need the exploration to not vanish so we set the decay\n",
    "                     #to zero.\n",
    "PRINT_EVERY = 10 #Print info about average reward every PRINT_EVERY\n",
    "\n",
    "ENV_NAME = \"CartPole-v0\" # Put here the gym env name you want to play with\n",
    "#check other environments to play with at https://gym.openai.com/envs/#mujoco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class replayBuffer(object):\n",
    "    def __init__(self, buffer_size, name_buffer=''):\n",
    "        self.buffer_size=buffer_size  #choose buffer size\n",
    "        self.num_exp=0\n",
    "        self.buffer=deque()\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience=(s, a, r, t, s2)\n",
    "        if self.num_exp < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.num_exp +=1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def count(self):\n",
    "        return self.num_exp\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if self.num_exp < batch_size:\n",
    "            batch=random.sample(self.buffer, self.num_exp)\n",
    "        else:\n",
    "            batch=random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s, a, r, t, s2 = map(np.stack, zip(*batch))\n",
    "\n",
    "        return s, a, r, t, s2\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_exp=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set GPU for faster training\n",
    "cuda = torch.cuda.is_available() #check for CUDA\n",
    "device   = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Job will run on {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecure\n",
    "3 fully connected layers with hyperbolic tanget as output activation function<br>\n",
    "Takes state and action as input and outputs the Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fanin_(size):\n",
    "    fan_in = size[0]\n",
    "    weight = 1./np.sqrt(fan_in)\n",
    "    return torch.Tensor(size).uniform_(-weight, weight)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, h1=H1, h2=H2, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "                \n",
    "        self.linear1 = nn.Linear(state_dim, h1)\n",
    "        self.linear1.weight.data = fanin_(self.linear1.weight.data.size())\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d(h1)\n",
    "        \n",
    "        self.linear2 = nn.Linear(h1+action_dim, h2)\n",
    "        self.linear2.weight.data = fanin_(self.linear2.weight.data.size())\n",
    "                \n",
    "        self.linear3 = nn.Linear(h2, 1)\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = self.linear1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(torch.cat([x,action],1))\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class Actor(nn.Module): \n",
    "    def __init__(self, state_dim, action_dim, h1=H1, h2=H2, init_w=0.003):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        #self.bn0 = nn.BatchNorm1d(state_dim)\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_dim, h1)\n",
    "        self.linear1.weight.data = fanin_(self.linear1.weight.data.size())\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d(h1)\n",
    "        \n",
    "        self.linear2 = nn.Linear(h1, h2)\n",
    "        self.linear2.weight.data = fanin_(self.linear2.weight.data.size())\n",
    "        \n",
    "        #self.bn2 = nn.BatchNorm1d(h2)\n",
    "        \n",
    "        self.linear3 = nn.Linear(h2, action_dim)\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, state):\n",
    "        #state = self.bn0(state)\n",
    "        x = self.linear1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.tanh(x)\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state  = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = self.forward(state)\n",
    "        return action.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu=0, sigma=0.2, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def _action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NormalizedEnv(gym.make(ENV_NAME))\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# for discrete action spaces\n",
    "action_dim = env.action_space.n\n",
    "#action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"State dim: {}, Action dim: {}\".format(state_dim, action_dim))\n",
    "\n",
    "noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "critic  = Critic(state_dim, action_dim).to(device)\n",
    "actor = Actor(state_dim, action_dim).to(device)\n",
    "\n",
    "target_critic  = Critic(state_dim, action_dim).to(device)\n",
    "target_actor = Actor(state_dim, action_dim).to(device)\n",
    "\n",
    "for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "q_optimizer  = opt.Adam(critic.parameters(),  lr=LRC)#, weight_decay=0.01)\n",
    "policy_optimizer = opt.Adam(actor.parameters(), lr=LRA)\n",
    "\n",
    "MSE = nn.MSELoss()\n",
    "\n",
    "memory = replayBuffer(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "def subplot(R, P, Q, S):\n",
    "    r = list(zip(*R))\n",
    "    p = list(zip(*P))\n",
    "    q = list(zip(*Q))\n",
    "    s = list(zip(*S))\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,15))\n",
    "\n",
    "    ax[0, 0].plot(list(r[1]), list(r[0]), 'r') #row=0, col=0\n",
    "    ax[1, 0].plot(list(p[1]), list(p[0]), 'b') #row=1, col=0\n",
    "    ax[0, 1].plot(list(q[1]), list(q[0]), 'g') #row=0, col=1\n",
    "    ax[1, 1].plot(list(s[1]), list(s[0]), 'k') #row=1, col=1\n",
    "    ax[0, 0].title.set_text('Reward')\n",
    "    ax[1, 0].title.set_text('Policy loss')\n",
    "    ax[0, 1].title.set_text('Q loss')\n",
    "    ax[1, 1].title.set_text('Max steps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward = []\n",
    "plot_policy = []\n",
    "plot_q = []\n",
    "plot_steps = []\n",
    "\n",
    "\n",
    "best_reward = -np.inf\n",
    "saved_reward = -np.inf\n",
    "saved_ep = 0\n",
    "average_reward = 0\n",
    "global_step = 0\n",
    "#s = deepcopy(env.reset())\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    #print(episode)\n",
    "    s = deepcopy(env.reset())\n",
    "    #noise.reset()\n",
    "\n",
    "    ep_reward = 0.\n",
    "    ep_q_value = 0.\n",
    "    step=0\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        #loss=0\n",
    "        global_step +=1\n",
    "        epsilon -= epsilon_decay\n",
    "        #actor.eval()\n",
    "        a = actor.get_action(s)\n",
    "        #actor.train()\n",
    "\n",
    "        a += noise()*max(0, epsilon)\n",
    "        a = np.clip(a, -1., 1.)\n",
    "        s2, reward, terminal, info = env.step(a)\n",
    "\n",
    "\n",
    "        memory.add(s, a, reward, terminal,s2)\n",
    "\n",
    "        #keep adding experiences to the memory until there are at least minibatch size samples\n",
    "        \n",
    "        if memory.count() > buffer_start:\n",
    "            s_batch, a_batch, r_batch, t_batch, s2_batch = memory.sample(BATCH_SIZE)\n",
    "\n",
    "            s_batch = torch.FloatTensor(s_batch).to(device)\n",
    "            a_batch = torch.FloatTensor(a_batch).to(device)\n",
    "            r_batch = torch.FloatTensor(r_batch).unsqueeze(1).to(device)\n",
    "            t_batch = torch.FloatTensor(np.float32(t_batch)).unsqueeze(1).to(device)\n",
    "            s2_batch = torch.FloatTensor(s2_batch).to(device)\n",
    "            \n",
    "            \n",
    "            #compute loss for critic\n",
    "            a2_batch = target_actor(s2_batch)\n",
    "            target_q = target_critic(s2_batch, a2_batch) #detach to avoid updating target\n",
    "            y = r_batch + (1.0 - t_batch) * GAMMA * target_q.detach()\n",
    "            q = critic(s_batch, a_batch)\n",
    "            \n",
    "            q_optimizer.zero_grad()\n",
    "            q_loss = MSE(q, y) #detach to avoid updating target\n",
    "            q_loss.backward()\n",
    "            q_optimizer.step()\n",
    "            \n",
    "            #compute loss for actor\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss = -critic(s_batch, actor(s_batch))\n",
    "            policy_loss = policy_loss.mean()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "            \n",
    "            #soft update of the frozen target networks\n",
    "            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - TAU) + param.data * TAU\n",
    "                )\n",
    "\n",
    "            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - TAU) + param.data * TAU\n",
    "                )\n",
    "\n",
    "        s = deepcopy(s2)\n",
    "        ep_reward += reward\n",
    "\n",
    "\n",
    "        #if terminal:\n",
    "        #    noise.reset()\n",
    "        #    break\n",
    "\n",
    "    try:\n",
    "        plot_reward.append([ep_reward, episode+1])\n",
    "        plot_policy.append([policy_loss.data, episode+1])\n",
    "        plot_q.append([q_loss.data, episode+1])\n",
    "        plot_steps.append([step+1, episode+1])\n",
    "    except:\n",
    "        continue\n",
    "    average_reward += ep_reward\n",
    "    \n",
    "    if ep_reward > best_reward:\n",
    "        torch.save(actor.state_dict(), 'best_model_pendulum.pkl') #Save the actor model for future testing\n",
    "        best_reward = ep_reward\n",
    "        saved_reward = ep_reward\n",
    "        saved_ep = episode+1\n",
    "\n",
    "    if (episode % PRINT_EVERY) == (PRINT_EVERY-1):    # print every print_every episodes\n",
    "        subplot(plot_reward, plot_policy, plot_q, plot_steps)\n",
    "        print('[%6d episode, %8d total steps] average reward for past {} iterations: %.3f'.format(PRINT_EVERY) %\n",
    "              (episode + 1, global_step, average_reward / PRINT_EVERY))\n",
    "        print(\"Last model saved with reward: {:.2f}, at episode {}.\".format(saved_reward, saved_ep))\n",
    "        average_reward = 0 #reset average reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
