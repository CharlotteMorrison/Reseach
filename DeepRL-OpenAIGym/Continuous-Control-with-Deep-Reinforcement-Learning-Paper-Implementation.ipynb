{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy\n",
    "import torch\n",
    "from torch import nn #needed for building neural networks\n",
    "import torch.nn.functional as F #needed for activation functions\n",
    "import torch.optim as opt #needed for optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement with a sum tree backed by an array so priority experience replay can be added.\n",
    "\n",
    "# TODO: add priority experience replay \n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "    \n",
    "    # returns the remaining capacity of the sumtree\n",
    "    def size(self):\n",
    "        return self.write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class replayBuffer:\n",
    "    e = 0.01\n",
    "    a2 = .6\n",
    "    \n",
    "    def __init__(self, capacity):        \n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "        \n",
    "    def _get_priority(self, error):\n",
    "        return (error + self.e) ** self.a2\n",
    "\n",
    "    def add(self, error, s, a, r, t, s2):\n",
    "        experience=(s, a, r, t, s2)\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add(p, experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        segment = self.tree.total()/batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            \n",
    "            s = random.uniform(a,b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append((idx,data))\n",
    "        \n",
    "        loc, batch = map(np.stack, zip(*batch))\n",
    "        s, a, r, t, s2  = map(np.stack, zip(*batch))\n",
    "        \n",
    "        return s, a, r, t, s2\n",
    "    \n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)   \n",
    "        \n",
    "    def size(self):\n",
    "        size = self.tree.size()\n",
    "        return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 fully connected layers with hyperbolic tanget as output activation function\n",
    "# Takes state and action as input and outputs the Q-value\n",
    "\n",
    "layer_1=400   #neurons of 1st layers\n",
    "layer_2=300   #neurons of 2nd layers\n",
    "\n",
    "def fanin_(size):\n",
    "    fan_in = size[0]\n",
    "    weight = 1./np.sqrt(fan_in)\n",
    "    return torch.Tensor(size).uniform_(-weight, weight)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, h1=layer_1, h2=layer_2, init_w=3e-3):\n",
    "        '''create the fully connected layers\n",
    "           nn.Linear(# input nodes, # next layer nodes)\n",
    "        '''\n",
    "        super(Critic, self).__init__()\n",
    "                \n",
    "        self.linear1 = nn.Linear(state_dim, h1)\n",
    "        self.linear1.weight.data = fanin_(self.linear1.weight.data.size())\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d(h1)\n",
    "        \n",
    "        self.linear2 = nn.Linear(h1+action_dim, h2)\n",
    "        self.linear2.weight.data = fanin_(self.linear2.weight.data.size())\n",
    "                \n",
    "        self.linear3 = nn.Linear(h2, 1)\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        '''the input data is defined as x at each step\n",
    "            x is replaced at each stage, feeding it to the next level\n",
    "        '''\n",
    "        print ('action 1 ' + str(action.size()))\n",
    "        print ('state 1  ' + str(state.size()))\n",
    "        x = self.linear1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(torch.cat([x,action],dim=1)) \n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(nn.Module): \n",
    "    def __init__(self, state_dim, action_dim, h1=layer_1, h2=layer_2, init_w=0.003):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        #self.bn0 = nn.BatchNorm1d(state_dim)\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_dim, h1)\n",
    "        self.linear1.weight.data = fanin_(self.linear1.weight.data.size())\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d(h1)\n",
    "        \n",
    "        self.linear2 = nn.Linear(h1, h2)\n",
    "        self.linear2.weight.data = fanin_(self.linear2.weight.data.size())\n",
    "        \n",
    "        #self.bn2 = nn.BatchNorm1d(h2)\n",
    "        \n",
    "        self.linear3 = nn.Linear(h2, action_dim)\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, state):\n",
    "        #state = self.bn0(state)\n",
    "        x = self.linear1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.tanh(x)\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state  = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = self.forward(state)\n",
    "        return action.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def _action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dim: 4, Action dim: 2\n",
      "Episode Finishd afer 18 timesteps.\n",
      "Episode Finishd afer 15 timesteps.\n",
      "Episode Finishd afer 14 timesteps.\n",
      "Episode Finishd afer 16 timesteps.\n",
      "if loop action\n",
      "torch.Size([64])\n",
      "action 1 torch.Size([64, 2])\n",
      "state 1  torch.Size([64, 4])\n",
      "action 1 torch.Size([64])\n",
      "state 1  torch.Size([64, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Tensors must have same number of dimensions: got 2 and 1 at /pytorch/aten/src/TH/generic/THTensor.cpp:702",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-320-d72712b73d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mtarget_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mq_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_rl/deep_rl_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-317-70cec6a3cbe7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Tensors must have same number of dimensions: got 2 and 1 at /pytorch/aten/src/TH/generic/THTensor.cpp:702"
     ]
    }
   ],
   "source": [
    "### NEED TO IMPLEMENT THE ERROR###\n",
    "\n",
    "### Set Variables ###\n",
    "environment = 'CartPole-v0'\n",
    "error = 0  \n",
    "buffer_size = 1000 # replace with 1000000 after testing\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "lra = 0.0001\n",
    "lrc = 0.001\n",
    "\n",
    "\n",
    "\n",
    "simulations = 50 # replace with 50000 after testing\n",
    "steps = 200\n",
    "\n",
    "# create the memory replay\n",
    "memory = replayBuffer(buffer_size)\n",
    "\n",
    "# set up mean square error\n",
    "MSE = nn.MSELoss()\n",
    "\n",
    "# set up the gym space\n",
    "env = gym.make(environment)\n",
    "\n",
    "                    \n",
    "# get the shape of the environment\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# for discrete action spaces\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# for continuous action spaces\n",
    "# action_dim = env.action_space.shape[0]\n",
    "\n",
    "# set up training (actor and critic)\n",
    "critic  = Critic(state_dim, action_dim).to(device)\n",
    "actor = Actor(state_dim, action_dim).to(device)\n",
    "\n",
    "target_critic  = Critic(state_dim, action_dim).to(device)\n",
    "target_actor = Actor(state_dim, action_dim).to(device)\n",
    "\n",
    "q_optimizer  = opt.Adam(critic.parameters(),  lr=lrc)#, weight_decay=0.01)\n",
    "policy_optimizer = opt.Adam(actor.parameters(), lr=lra)\n",
    "\n",
    "for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "print(\"State dim: {}, Action dim: {}\".format(state_dim, action_dim))\n",
    "\n",
    "for i in range(simulations): # number of times to run the simulation\n",
    "    \n",
    "    # get the initial observation at simulation start\n",
    "    initial_observation = env.reset()\n",
    "    \n",
    "    # set reward\n",
    "    ep_reward = 0.\n",
    "    \n",
    "    for t in range (steps): # number of time steps in each simulation \n",
    "        \n",
    "        # uncomment to see the simulation\n",
    "        # env.render()  \n",
    "        \n",
    "        # select action (can implement a policy here)\n",
    "        action = env.action_space.sample() \n",
    "        \n",
    "        # execute action and observe reward, and next state\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        #TODO: add noise to action\n",
    "        \n",
    "        \n",
    "        \n",
    "        #TODO: add initial error\n",
    "        \n",
    "        \n",
    "        \n",
    "        # store current step, and next step \n",
    "        memory.add(error, initial_observation, action, reward, done, new_observation)  \n",
    "        \n",
    "       \n",
    "        # sample mini-batch of transitions if the memory is bigger than a batch\n",
    "        if (memory.size() >= batch_size):\n",
    "            \n",
    "            #TODO: implement priority sampling.\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            state_1, action, reward, status, state_2 = memory.sample(batch_size)\n",
    "            \n",
    "            # copy data into tensors\n",
    "            state_1 = torch.FloatTensor(state_1).to(device)\n",
    "            action = torch.FloatTensor(action).to(device)\n",
    "            reward = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "            status = torch.FloatTensor(np.float32(status)).unsqueeze(1).to(device)\n",
    "            state_2 = torch.FloatTensor(state_2).to(device)         \n",
    "            print('if loop action')\n",
    "            print(action.size())\n",
    "            \n",
    "            # compute the loss for the critic\n",
    "            action_2 = target_actor(state_2)\n",
    "            target_q = target_critic(state_2, action_2)\n",
    "            y = reward + (1.0 - status) * gamma * target_q.detach()\n",
    "            q = critic(state_1, action)\n",
    "            \n",
    "            q_optimizer.zero_grad()\n",
    "            q_loss = MSE(q, y) #detach to avoid updating target\n",
    "            q_loss.backward()\n",
    "            q_optimizer.step()            \n",
    "            \n",
    "            #compute loss for actor\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss = -critic(state_1, actor(state_1))\n",
    "            policy_loss = policy_loss.mean()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "            \n",
    "            #soft update of the frozen target networks\n",
    "            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - tau) + param.data * tau\n",
    "                )\n",
    "\n",
    "            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - tau) + param.data * tau\n",
    "                )\n",
    "        # make next step the current step\n",
    "        initial_observation = new_observation\n",
    "        ep_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print('Episode Finishd afer {} timesteps.'.format(t+1))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on cpu, not set up for GPU as of yet...\n",
    "cuda = torch.cuda.is_available() #check for CUDA\n",
    "device   = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Job will run on {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
